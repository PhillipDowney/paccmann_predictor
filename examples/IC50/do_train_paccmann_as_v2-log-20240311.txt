      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann:Training about to start...

INFO:paccmann:example_params.json
INFO:paccmann:== Epoch [0/200] ==
cuda
Traceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 344, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 248, in main
    y_hat, pred_dict = model(torch.squeeze(smiles.to(device)), gep.to(device))
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/paccmann_predictor/models/paccmann_v2.py", line 259, in forward
    e, a = self.molecule_attention_layers[ind](
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/paccmann_predictor/utils/layers.py", line 242, in forward
    context_attention = self.context_hidden_projection(
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 21.99 GiB of which 437.88 MiB is free. Including non-PyTorch memory, this process has 21.54 GiB memory in use. Of the allocated memory 19.34 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ echo $PYT
$PYTHONPATH               $PYTORCH_CUDA_ALLOC_CONF  
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ echo $PYT
$PYTHONPATH               $PYTORCH_CUDA_ALLOC_CONF  
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ echo $PYTORCH_CUDA_ALLOC_CONF 
expandable_segments:True
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,backend=cudaMallocAsync
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann.sh 
INFO:paccmann:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann:Device for data loader is None and for model is cuda
INFO:paccmann:Number of parameters 47554353
INFO:paccmann:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann:Training about to start...

INFO:paccmann:example_params.json
INFO:paccmann:== Epoch [0/200] ==
cuda
Traceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 344, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 248, in main
    y_hat, pred_dict = model(torch.squeeze(smiles.to(device)), gep.to(device))
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/paccmann_predictor/models/paccmann_v2.py", line 259, in forward
    e, a = self.molecule_attention_layers[ind](
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/paccmann_predictor/utils/layers.py", line 242, in forward
    context_attention = self.context_hidden_projection(
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 21.99 GiB of which 437.88 MiB is free. Including non-PyTorch memory, this process has 21.54 GiB memory in use. Of the allocated memory 19.34 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann_v2:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann_v2:Device for data loader is None and for model is cuda
INFO:paccmann_v2:Number of parameters 45102129
INFO:paccmann_v2:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann_v2:Training about to start...

INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [0/10] ==
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
cuda
^CTraceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 344, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 251, in main
    loss.backward()
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ cd data
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl/data$ ls
2128_genes.pkl  README.md  drug_sensitivity  gdsc_transcriptomics_for_conditional_generation.pkl  gene_expression  raw  smiles  smiles_language_chembl_gdsc_ccle.pkl  splitted_data
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl/data$ cd 
(openad-train) deanelzinga@cde-ubuntu-g5-3:~$ cd paccmann_rl/
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl$ cd data
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl/data$ ls
2128_genes.pkl  README.md  drug_sensitivity  gdsc_transcriptomics_for_conditional_generation.pkl  gene_expression  raw  smiles  smiles_language_chembl_gdsc_ccle.pkl  splitted_data
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl/data$ cd ..
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl$ ls
LICENSE  README.md  code  conda.yml  data  public  requirements.txt  tox21  trained_models
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl$ cd code
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl/code$ ls
paccmann_predictor
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl/code$ tree -L 3
.
└── paccmann_predictor
    ├── LICENSE
    ├── README.md
    ├── assets
    │   └── paccmann.png
    ├── build
    │   ├── bdist.linux-x86_64
    │   └── lib
    ├── data -> ../../data
    ├── examples
    │   ├── IC50
    │   └── affinity
    ├── models
    │   ├── de-pretr-20240214-1835
    │   └── de-pretr-20240215-1119
    ├── paccmann_predictor
    │   ├── __init__.py
    │   ├── __pycache__
    │   ├── models
    │   └── utils
    ├── paccmann_predictor.egg-info
    │   ├── PKG-INFO
    │   ├── SOURCES.txt
    │   ├── dependency_links.txt
    │   ├── not-zip-safe
    │   ├── requires.txt
    │   └── top_level.txt
    ├── run-train_paccmann.sh
    ├── setup.py
    └── trained_models -> ../../trained_models/

18 directories, 12 files
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl/code$ cd ..
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl$ ls
LICENSE  README.md  code  conda.yml  data  public  requirements.txt  tox21  trained_models
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl$ tree trained_models/
trained_models/
├── paccmann
│   ├── README.md
│   ├── model_params.json
│   ├── smiles_language.pkl
│   ├── token_count.json
│   ├── tokenizer_config.json
│   ├── vocab.json
│   └── weights
│       ├── best_pearson_mca.pt
│       ├── best_rmse_mca.pt
│       └── done_training_mca.pt
├── pvae
│   ├── model_params.json
│   └── weights
│       ├── best_both_decoder.pt
│       ├── best_both_encoder.pt
│       ├── best_both_vae.pt
│       ├── best_kl_decoder.pt
│       ├── best_kl_encoder.pt
│       ├── best_kl_vae.pt
│       ├── best_rec_decoder.pt
│       ├── best_rec_encoder.pt
│       ├── best_rec_vae.pt
│       ├── done_training_decoder.pt
│       ├── done_training_encoder.pt
│       └── done_training_vae.pt
└── svae
    ├── loss_tracker.json
    ├── model_params.json
    └── weights
        ├── best_kld.pt
        ├── best_loss.pt
        └── best_rec.pt

6 directories, 27 files
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/paccmann_rl$ cd
(openad-train) deanelzinga@cde-ubuntu-g5-3:~$ find . -name 
find: missing argument to `-name'
(openad-train) deanelzinga@cde-ubuntu-g5-3:~$ find . -name '*single*'
./phillipdowney/paccmann_rl/code/paccmann_predictor/single_pytorch_model
./gt4sd-molformer/apex/tests/distributed/synced_batchnorm/python_single_gpu_unit_test.py
./gt4sd-molformer/apex/tests/distributed/synced_batchnorm/single_gpu_unit_test.py
./molformer-torch2/apex/tests/distributed/synced_batchnorm/__pycache__/python_single_gpu_unit_test.cpython-39-pytest-8.0.0.pyc
./molformer-torch2/apex/tests/distributed/synced_batchnorm/__pycache__/single_gpu_unit_test.cpython-39-pytest-8.0.0.pyc
./molformer-torch2/apex/tests/distributed/synced_batchnorm/python_single_gpu_unit_test.py
./molformer-torch2/apex/tests/distributed/synced_batchnorm/single_gpu_unit_test.py
./Open-AD-Model-Service/openad-model-training/gt4sd_paccmann/single_pytorch_model.zip
./.vscode-server/extensions/ms-python.vscode-pylance-2024.2.3/dist/typeshed-fallback/stubs/stripe/stripe/api_resources/abstract/singleton_api_resource.pyi
./.vscode-server/extensions/ms-python.vscode-pylance-2024.2.3/dist/typeshed-fallback/stubs/openpyxl/openpyxl/compat/singleton.pyi
./.vscode-server/extensions/ms-python.vscode-pylance-2024.2.3/dist/typeshed-fallback/stubs/influxdb-client/influxdb_client/domain/line_plus_single_stat_properties.pyi
./.vscode-server/extensions/ms-python.vscode-pylance-2024.2.3/dist/typeshed-fallback/stubs/influxdb-client/influxdb_client/domain/single_stat_view_properties.pyi
./.vscode-server/extensions/ms-python.vscode-pylance-2024.2.3/dist/typeshed-fallback/stubs/singledispatch
./.vscode-server/extensions/ms-python.vscode-pylance-2024.2.3/dist/typeshed-fallback/stubs/singledispatch/singledispatch.pyi
./.vscode-server/extensions/ms-vsliveshare.vsliveshare-1.0.5918/audio/Scroll_Wheel_singleClick_2.wav
./.vscode-server/extensions/ms-vsliveshare.vsliveshare-1.0.5918/audio/Scroll_Wheel_singleClick_1.wav
./.vscode-server/extensions/ms-python.python-2024.2.1/pythonFiles/lib/jedilsp/jedi/third_party/typeshed/third_party/2and3/singledispatch.pyi
./.vscode-server/extensions/ms-python.python-2024.2.1/pythonFiles/lib/python/debugpy/common/singleton.py
./.vscode-server/extensions/ms-python.debugpy-2024.2.0-linux-x64/bundled/libs/debugpy/common/singleton.py
^C
(openad-train) deanelzinga@cde-ubuntu-g5-3:~$ cd phillipdowney/
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney$ cd paccmann_rl/code/paccmann_predictor/
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl/code/paccmann_predictor$ ls
LICENSE  README.md  assets  examples  models  paccmann_predictor  results  results.csv  run-test_paccmann.sh  run-training-log.txt  setup.py  single_pytorch_model
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl/code/paccmann_predictor$ mv single_pytorch_model ~/phillipdowney/paccmann_rl/data/
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl/code/paccmann_predictor$ cd ~/phillipdowney/
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney$ cd paccmann_rl/
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ ls
LICENSE  README.md  code  conda.yml  data  models  requirements.txt  run-train_paccmann.sh  run-train_paccmann_v2.sh
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ ls data
2128_genes.pkl  drug_sensitivity                                     gene_expression  single_pytorch_model  smiles_language_chembl_gdsc_ccle.pkl
README.md       gdsc_transcriptomics_for_conditional_generation.pkl  raw              smiles                splitted_data
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann_v2:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann_v2:Device for data loader is None and for model is cuda
INFO:paccmann_v2:Number of parameters 45102129
INFO:paccmann_v2:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann_v2:Training about to start...

INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [0/10] ==
^CTraceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 343, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 246, in main
    for ind, (smiles, gep, y) in enumerate(train_loader):
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/drug_sensitivity_dataset.py", line 244, in __getitem__
    gene_expression_tensor = self.gene_expression_dataset.get_item_from_key(
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/base_dataset.py", line 79, in get_item_from_key
    return self.__getitem__(self.get_index(key))
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/base_dataset.py", line 69, in __getitem__
    return self.dataset[index]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_table_dataset.py", line 178, in __getitem__
    return torch.tensor(self.dataset[index], dtype=self.dtype)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/dataset.py", line 335, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/dataframe_dataset.py", line 32, in __getitem__
    return self.df.iloc[index].values
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pandas/core/indexing.py", line 1191, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pandas/core/indexing.py", line 1754, in _getitem_axis
    return self.obj._ixs(key, axis=axis)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pandas/core/frame.py", line 3984, in _ixs
    new_mgr = self._mgr.fast_xs(i)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pandas/core/internals/managers.py", line 984, in fast_xs
    dtype = interleaved_dtype([blk.dtype for blk in self.blocks])
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pandas/core/internals/managers.py", line 984, in <listcomp>
    dtype = interleaved_dtype([blk.dtype for blk in self.blocks])
KeyboardInterrupt
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_single_pytorch_model.sh 
INFO:paccmann:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann:Device for data loader is None and for model is cuda
INFO:paccmann:Number of parameters 47554353
INFO:paccmann:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=560, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=560, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann:Training about to start...

INFO:paccmann:example_params.json
INFO:paccmann:== Epoch [0/200] ==
Traceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 343, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 247, in main
    y_hat, pred_dict = model(torch.squeeze(smiles.to(device)), gep.to(device))
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/paccmann_predictor/models/paccmann_v2.py", line 259, in forward
    e, a = self.molecule_attention_layers[ind](
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/paccmann_predictor/utils/layers.py", line 242, in forward
    context_attention = self.context_hidden_projection(
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.04 GiB. GPU 0 has a total capacity of 21.99 GiB of which 437.88 MiB is free. Including non-PyTorch memory, this process has 21.54 GiB memory in use. Of the allocated memory 19.34 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann_v2:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann_v2:Device for data loader is None and for model is cuda
INFO:paccmann_v2:Number of parameters 45102129
INFO:paccmann_v2:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann_v2:Training about to start...

INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [0/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [1/10], loss: 0.01691. This took 232.5 secs.
Traceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 343, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 281, in main
    test_pearson_a = pearsonr(torch.Tensor(predictions), torch.Tensor(labels))
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/paccmann_predictor/utils/loss_functions.py", line 24, in pearsonr
    raise ValueError(' x and y must be 1D Tensors.')
ValueError:  x and y must be 1D Tensors.
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann_v2:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann_v2:Device for data loader is None and for model is cuda
INFO:paccmann_v2:Number of parameters 45102129
INFO:paccmann_v2:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann_v2:Training about to start...

INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [0/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [1/10], loss: 0.01619. This took 227.7 secs.
x.shape torch.Size([7424, 1])
y.shape torch.Size([7424, 1])
Traceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 343, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 281, in main
    test_pearson_a = pearsonr(torch.Tensor(predictions), torch.Tensor(labels))
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/paccmann_predictor/utils/loss_functions.py", line 24, in pearsonr
    raise ValueError(' x and y must be 1D Tensors.')
ValueError:  x and y must be 1D Tensors.
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
Traceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 343, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 129, in main
    train_dataset = DrugSensitivityDataset(
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/drug_sensitivity_dataset.py", line 140, in __init__
    device_warning(device)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/warnings.py", line 25, in device_warning
    raise ValueError(msg)
ValueError: Found deprecated argument: device=cpu. Device value will be ignored, all devices will be set to `cpu`. If you are using cuda devices, also remember to transfer tensors to the gpu before using a model __call__
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/warnings.py:26: UserWarning: Found deprecated argument: device=cpu. Device value will be ignored, all devices will be set to `cpu`. If you are using cuda devices, also remember to transfer tensors to the gpu before using a model __call__
  warn(msg)
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann_v2:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann_v2:Device for data loader is None and for model is cuda
INFO:paccmann_v2:Number of parameters 45102129
INFO:paccmann_v2:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann_v2:Training about to start...

INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [0/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [1/10], loss: 0.01874. This took 231.1 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [1/10], loss: 0.01338, Pearson: 0.348, RMSE: 0.116
/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py:310: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  "predictions": [float(p) for p in predictions],
INFO:paccmann_v2:        New best performance in "mse" with value : 0.0133846 in epoch: 0
INFO:paccmann_v2:        New best performance in "pearson" with value : 0.3479837 in epoch: 0
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [1/10] ==
^CTraceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 343, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 246, in main
    for ind, (smiles, gep, y) in enumerate(train_loader):
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/drug_sensitivity_dataset.py", line 244, in __getitem__
    gene_expression_tensor = self.gene_expression_dataset.get_item_from_key(
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/base_dataset.py", line 79, in get_item_from_key
    return self.__getitem__(self.get_index(key))
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/base_dataset.py", line 112, in __getattr__
    if k in super().__getattribute__('_delegatable'):
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/base_dataset.py", line 102, in _delegatable
    return [
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/base_dataset.py", line 107, in <listcomp>
    if self._delegation_filter(o)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/base_dataset.py", line 92, in _delegation_filter
    if method_name.startswith('_'):
KeyboardInterrupt
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann_v2:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann_v2:Device for data loader is None and for model is cuda
INFO:paccmann_v2:Found existing model, restoring now...
INFO:paccmann_v2:Number of parameters 45102129
INFO:paccmann_v2:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann_v2:Training about to start...

INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [0/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [1/10], loss: 0.01404. This took 228.5 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [1/10], loss: 0.01276, Pearson: 0.458, RMSE: 0.113
Traceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 343, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 313, in main
    if test_loss_a < min_loss:
TypeError: '<' not supported between instances of 'float' and 'str'
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann_v2:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann_v2:Device for data loader is None and for model is cuda
INFO:paccmann_v2:Found existing model, restoring now...
INFO:paccmann_v2:Number of parameters 45102129
INFO:paccmann_v2:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann_v2:Training about to start...

INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [0/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [1/10], loss: 0.01463. This took 235.3 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [1/10], loss: 0.01298, Pearson: 0.426, RMSE: 0.114
/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py:310: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  "predictions": [float(p) for p in predictions],
INFO:paccmann_v2:        New best performance in "mse" with value : 0.0129832 in epoch: 0
INFO:paccmann_v2:        New best performance in "pearson" with value : 0.4260980 in epoch: 0
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [1/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [2/10], loss: 0.01251. This took 260.3 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [2/10], loss: 0.01221, Pearson: 0.528, RMSE: 0.111
INFO:paccmann_v2:        New best performance in "mse" with value : 0.0122145 in epoch: 1
INFO:paccmann_v2:        New best performance in "pearson" with value : 0.5275847 in epoch: 1
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [2/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [3/10], loss: 0.01064. This took 258.0 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [3/10], loss: 0.00876, Pearson: 0.680, RMSE: 0.094
INFO:paccmann_v2:        New best performance in "mse" with value : 0.0087618 in epoch: 2
INFO:paccmann_v2:        New best performance in "pearson" with value : 0.6796026 in epoch: 2
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [3/10] ==
^CTraceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 343, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 248, in main
    loss = model.loss(y_hat, y.to(device))
KeyboardInterrupt
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann_v2:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann_v2:Device for data loader is None and for model is cuda
INFO:paccmann_v2:Found existing model, restoring now...
INFO:paccmann_v2:Number of parameters 45102129
INFO:paccmann_v2:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann_v2:Training about to start...

INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [0/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [1/10], loss: 0.00853. This took 234.3 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [1/10], loss: 0.00912, Pearson: 0.716, RMSE: 0.095
np.shape(predictions[0]) (1,)
/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py:310: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  "predictions": [float(p) for p in predictions],
INFO:paccmann_v2:        New best performance in "pearson" with value : 0.7163966 in epoch: 0
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [1/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [2/10], loss: 0.00763. This took 256.6 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [2/10], loss: 0.02444, Pearson: 0.550, RMSE: 0.156
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [2/10] ==
^CTraceback (most recent call last):
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 343, in <module>
    main(
  File "/home/deanelzinga/phillipdowney/paccmann_rl/./code/paccmann_predictor/examples/IC50/train_paccmann.py", line 246, in main
    for ind, (smiles, gep, y) in enumerate(train_loader):
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/drug_sensitivity_dataset.py", line 244, in __getitem__
    gene_expression_tensor = self.gene_expression_dataset.get_item_from_key(
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/base_dataset.py", line 79, in get_item_from_key
    return self.__getitem__(self.get_index(key))
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/base_dataset.py", line 69, in __getitem__
    return self.dataset[index]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_table_dataset.py", line 178, in __getitem__
    return torch.tensor(self.dataset[index], dtype=self.dtype)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/torch/utils/data/dataset.py", line 335, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/dataframe_dataset.py", line 32, in __getitem__
    return self.df.iloc[index].values
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pandas/core/indexing.py", line 1191, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pandas/core/indexing.py", line 1754, in _getitem_axis
    return self.obj._ixs(key, axis=axis)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pandas/core/frame.py", line 3984, in _ixs
    new_mgr = self._mgr.fast_xs(i)
  File "/opt/conda/envs/openad-train/lib/python3.10/site-packages/pandas/core/internals/managers.py", line 1002, in fast_xs
    result[rl] = blk.iget((i, loc))
KeyboardInterrupt
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ . run-train_paccmann_v2.sh 
INFO:paccmann_v2:Start data preprocessing...
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
ERROR:pytoda.datasets.smiles_dataset:Since you provided a smiles_language, the following parameters to this class will be ignored: canonical, augment, kekulize, all_bonds_explicit, selfies, sanitize, all_hs_explicit, remove_bonddir, remove_chirality, randomize, add_start_and_stop, padding, padding_length.
Here are the problems:
ERROR:pytoda.datasets.smiles_dataset:Provided arg padding:True does not match the smiles_language value: False NOTE: smiles_language value takes preference!!
ERROR:pytoda.datasets.smiles_dataset:To get rid of this, adapt the smiles_language *offline*, feed it ready for intended usage, and adapt the constructor args to be identical with their equivalents in the language object
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:472: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmin(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/_array_api.py:489: RuntimeWarning: All-NaN slice encountered
  return xp.asarray(numpy.nanmax(X, axis=axis))
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide
  updated_mean = (last_sum + new_sum) / updated_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide
  T = new_sum / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide
  new_unnormalized_variance -= correction**2 / new_sample_count
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:220: RuntimeWarning: All-NaN axis encountered
  maximum = np.nanmax(maximums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:221: RuntimeWarning: All-NaN axis encountered
  minimum = np.nanmin(minimums, axis=0)
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:225: RuntimeWarning: invalid value encountered in divide
  np.nansum(
/opt/conda/envs/openad-train/lib/python3.10/site-packages/pytoda/datasets/_csv_statistics.py:236: RuntimeWarning: invalid value encountered in divide
  np.nansum(
INFO:paccmann_v2:Training dataset has 69479 samples, test set has 7670.
INFO:paccmann_v2:Device for data loader is None and for model is cuda
INFO:paccmann_v2:Found existing model, restoring now...
INFO:paccmann_v2:Number of parameters 45102129
INFO:paccmann_v2:PaccMannV2(
  (loss_fn): MSELoss()
  (act_fn): ReLU()
  (smiles_embedding): Embedding(35, 16)
  (convolutional_layers): Sequential(
    (convolutional_0): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(3, 16), stride=(1, 1), padding=(1, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_1): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(5, 16), stride=(1, 1), padding=(2, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (convolutional_2): Sequential(
      (convolve): Conv2d(1, 64, kernel_size=(11, 16), stride=(1, 1), padding=(5, 0))
      (squeeze): Squeeze()
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
      (batch_norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (molecule_attention_layers): Sequential(
    (molecule_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_0_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=16, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_1_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_2_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_2): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (molecule_attention_3_head_3): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=64, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=1, out_features=64, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=2128, out_features=512, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (gene_attention_layers): Sequential(
    (gene_attention_0_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_0_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=16, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_1_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_2_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_0): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
    (gene_attention_3_head_1): ContextAttentionLayer(
      (individual_nonlinearity): Sequential()
      (reference_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_projection): Sequential(
        (projection): Linear(in_features=64, out_features=1, bias=True)
        (act_fn): Sequential()
      )
      (context_hidden_projection): Sequential(
        (projection): Linear(in_features=512, out_features=2128, bias=True)
        (act_fn): Sequential()
      )
      (alpha_projection): Sequential(
        (projection): Linear(in_features=1, out_features=1, bias=False)
        (squeeze): Squeeze()
        (temperature): Temperature()
        (softmax): Softmax(dim=1)
      )
    )
  )
  (batch_norm): BatchNorm1d(17856, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dense_layers): Sequential(
    (dense_0): Sequential(
      (projection): Linear(in_features=17856, out_features=1024, bias=True)
      (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (dense_1): Sequential(
      (projection): Linear(in_features=1024, out_features=512, bias=True)
      (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act_fn): ReLU()
      (dropout): Dropout(p=0.5, inplace=False)
    )
  )
  (final_dense): Sequential(
    (projection): Linear(in_features=512, out_features=1, bias=True)
    (sigmoidal): Sigmoid()
  )
)
INFO:paccmann_v2:Training about to start...

INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [0/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [1/10], loss: 0.00851. This took 230.4 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [1/10], loss: 0.00792, Pearson: 0.717, RMSE: 0.089
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:        New best performance in "mse" with value : 0.0079193 in epoch: 0
INFO:paccmann_v2:        New best performance in "pearson" with value : 0.7165483 in epoch: 0
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [1/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [2/10], loss: 0.00757. This took 253.7 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [2/10], loss: 0.00777, Pearson: 0.704, RMSE: 0.088
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:        New best performance in "mse" with value : 0.0077716 in epoch: 1
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [2/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [3/10], loss: 0.00715. This took 252.7 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [3/10], loss: 0.00578, Pearson: 0.793, RMSE: 0.076
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:        New best performance in "mse" with value : 0.0057763 in epoch: 2
INFO:paccmann_v2:        New best performance in "pearson" with value : 0.7931489 in epoch: 2
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [3/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [4/10], loss: 0.00669. This took 253.9 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [4/10], loss: 0.00612, Pearson: 0.786, RMSE: 0.078
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [4/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [5/10], loss: 0.00636. This took 252.0 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [5/10], loss: 0.00654, Pearson: 0.770, RMSE: 0.081
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [5/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [6/10], loss: 0.00622. This took 252.4 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [6/10], loss: 0.01141, Pearson: 0.678, RMSE: 0.107
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [6/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [7/10], loss: 0.00600. This took 252.9 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [7/10], loss: 0.00639, Pearson: 0.787, RMSE: 0.080
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [7/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [8/10], loss: 0.00586. This took 252.4 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [8/10], loss: 0.00606, Pearson: 0.819, RMSE: 0.078
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:        New best performance in "pearson" with value : 0.8193659 in epoch: 7
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [8/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [9/10], loss: 0.00566. This took 252.7 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [9/10], loss: 0.00610, Pearson: 0.779, RMSE: 0.078
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:paccmann_v2_params.json
INFO:paccmann_v2:== Epoch [9/10] ==
INFO:paccmann_v2:        **** TRAINING ****   Epoch [10/10], loss: 0.00572. This took 252.4 secs.
x.shape torch.Size([7424])
y.shape torch.Size([7424])
INFO:paccmann_v2:        **** TESTING **** Epoch [10/10], loss: 0.00524, Pearson: 0.814, RMSE: 0.072
np.shape(predictions[0]) (1,)
INFO:paccmann_v2:        New best performance in "mse" with value : 0.0052379 in epoch: 9
INFO:paccmann_v2:Overall best performances are: 
        Loss = 0.0052 in epoch 9         (Pearson was 0.813814) 
        Pearson = 0.8194 in epoch 7      (Loss was 0.006059)
INFO:paccmann_v2:Done with training, models saved, shutting down.
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdowney/paccmann_rl$ sudo shutdown
Shutdown scheduled for Tue 2024-03-12 03:43:15 UTC, use 'shutdown -c' to cancel.
(openad-train) deanelzinga@cde-ubuntu-g5-3:~/phillipdo